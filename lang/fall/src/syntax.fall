tokenizer {
  eq '='
  pipe '|'
  star '*'
  question '?'
  dot '.'
  comma ','
  hash '#'
  lbrace '{'
  rbrace '}'
  lbrack '['
  rbrack ']'
  langle '<'
  rangle '>'
  lparen '('
  rparen ')'
  kw_node 'node'
  kw_class 'class'
  kw_tokenizer 'tokenizer'
  kw_rule 'rule'
  kw_verbatim 'verbatim'
  kw_ast 'ast'
  kw_pub 'pub'

  whitespace r"\s+"
  simple_string r#"'([^'\\]|\\.)*'"#
  hash_string r"r#*" 'parse_raw_string'
  ident r"\w+"
}

pub rule fall_file {
  tokenizer_def
  <rep { <skip_until {'pub' | 'rule' | '#'}> syn_rule }>
  <opt verbatim_def>
  <opt ast_def>
}

pub rule tokenizer_def {
  'tokenizer' <commit> '{' <rep lex_rule> '}'
}

pub rule lex_rule { ident <commit> string <opt string> }

pub rule syn_rule {
  <opt attributes> <opt 'pub'> 'rule'
  <commit> ident block_expr
}

pub rule attributes {
    '#' '['
     <rep {ident { ',' | &']' } }>
     ']'
}

pub rule string { simple_string | hash_string }

pub rule verbatim_def { 'verbatim' hash_string }

pub rule ast_def { 'ast' '{' <rep node_or_class> '}' }

rule node_or_class {
 <skip_until {'node' | 'class'}> {ast_node_def | ast_class_def}
}

pub rule ast_node_def {
  'node' <commit> ident '{' <rep method_def> '}'
}

pub rule ast_class_def {
  'class' <commit> ident '{' <layer block_body <rep ident>> '}'
}

pub rule method_def { ident ast_selector }
pub rule ast_selector { ident <opt ast_selector_suffix> }
rule ast_selector_suffix { '*' | '?' | '.' ident }

rule expr { call_expr | ref_expr | block_expr }
pub rule ref_expr { ident | simple_string }
pub rule call_expr { '<' ident <rep expr> '>' }
pub rule seq_expr { <rep expr> }
pub rule block_expr {
  '{' <layer block_body {<opt seq_expr> <rep {'|' seq_expr}>}> '}'
}

rule block_body { <rep balanced> }
rule balanced {
  '{' <commit> block_body '}'
| <not '}'>
}


verbatim r#########"

fn parse_raw_string(s: &str) -> Option<usize> {
    let quote_start = s.find('"').unwrap();
    let q_hashes = concat!('"', "######", "######", "######", "######", "######");
    let closing = &q_hashes[..quote_start];
    s[quote_start + 1..].find(closing).map(|i| i + quote_start + 1 + closing.len())
}

"#########

ast {
  node fall_file {
    tokenizer_def tokenizer_def?
    syn_rules syn_rule*
    verbatim_def verbatim_def?
    ast_def ast_def?
  }

  node tokenizer_def {
    lex_rules lex_rule*
  }

  node lex_rule {
    node_type IDENT.text
  }

  node syn_rule {
    attributes attributes
    name IDENT.text
    body expr
  }

  node attributes {
  }

  node verbatim_def { }

  node ast_def {
    ast_nodes ast_node_def*
    ast_classes ast_class_def*
  }

  node ast_node_def {
    name IDENT.text
    methods method_def*
  }

  node ast_class_def {
  }

  node method_def {
    name IDENT.text
  }

  node ref_expr {  }

  node call_expr {
    fn_name IDENT.text
    args expr*
  }

  node seq_expr {
    parts expr*
  }

  node block_expr {
    alts expr*
  }

  class expr {
    ref_expr call_expr seq_expr block_expr
  }
}
